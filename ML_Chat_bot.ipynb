{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRSOXQVIbwvez3mZ9zp1CR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balakrishna-24/NLP-ML-Chatbot/blob/main/ML_Chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfBzCaje_-So",
        "outputId": "ddd0645a-9578-4c92-9d51-545cd394251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "zip_file_path=\"/content/ap.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path,'r') as zip:\n",
        "  zip.extractall('/content')\n",
        "\n",
        "excel=\"/content/customer_support_tickets.csv\"\n",
        "import pandas as pd\n",
        "df=pd.read_csv(excel)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyvVjtKFAiF5",
        "outputId": "2ac069ee-c50b-413e-fb5e-74ad32c17788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Ticket ID        Customer Name              Customer Email  Customer Age  \\\n",
            "0          1        Marisa Obrien  carrollallison@example.com            32   \n",
            "1          2         Jessica Rios    clarkeashley@example.com            42   \n",
            "2          3  Christopher Robbins   gonzalestracy@example.com            48   \n",
            "3          4     Christina Dillon    bradleyolson@example.org            27   \n",
            "4          5    Alexander Carroll     bradleymark@example.com            67   \n",
            "\n",
            "  Customer Gender Product Purchased Date of Purchase      Ticket Type  \\\n",
            "0           Other        GoPro Hero       2021-03-22  Technical issue   \n",
            "1          Female       LG Smart TV       2021-05-22  Technical issue   \n",
            "2           Other          Dell XPS       2020-07-14  Technical issue   \n",
            "3          Female  Microsoft Office       2020-11-13  Billing inquiry   \n",
            "4          Female  Autodesk AutoCAD       2020-02-04  Billing inquiry   \n",
            "\n",
            "             Ticket Subject  \\\n",
            "0             Product setup   \n",
            "1  Peripheral compatibility   \n",
            "2           Network problem   \n",
            "3            Account access   \n",
            "4                 Data loss   \n",
            "\n",
            "                                  Ticket Description  \\\n",
            "0  I'm having an issue with the {product_purchase...   \n",
            "1  I'm having an issue with the {product_purchase...   \n",
            "2  I'm facing a problem with my {product_purchase...   \n",
            "3  I'm having an issue with the {product_purchase...   \n",
            "4  I'm having an issue with the {product_purchase...   \n",
            "\n",
            "               Ticket Status                                     Resolution  \\\n",
            "0  Pending Customer Response                                            NaN   \n",
            "1  Pending Customer Response                                            NaN   \n",
            "2                     Closed   Case maybe show recently my computer follow.   \n",
            "3                     Closed  Try capital clearly never color toward story.   \n",
            "4                     Closed                    West decision evidence bit.   \n",
            "\n",
            "  Ticket Priority Ticket Channel  First Response Time   Time to Resolution  \\\n",
            "0        Critical   Social media  2023-06-01 12:15:36                  NaN   \n",
            "1        Critical           Chat  2023-06-01 16:45:38                  NaN   \n",
            "2             Low   Social media  2023-06-01 11:14:38  2023-06-01 18:05:38   \n",
            "3             Low   Social media  2023-06-01 07:29:40  2023-06-01 01:57:40   \n",
            "4             Low          Email  2023-06-01 00:12:42  2023-06-01 19:53:42   \n",
            "\n",
            "   Customer Satisfaction Rating  \n",
            "0                           NaN  \n",
            "1                           NaN  \n",
            "2                           3.0  \n",
            "3                           3.0  \n",
            "4                           1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('customer_support_tickets.csv')\n",
        "\n",
        "ndf= df[['Ticket Description', 'Ticket Type']]\n",
        "ndf.to_csv('nlpa.csv', index=False)"
      ],
      "metadata": {
        "id": "7FcKFcICAqF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('nlpa.csv')\n",
        "print(\"Dataset Shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tVMOqm6Eo44",
        "outputId": "3ad4ca7e-8e0c-4c6f-86dc-6075a0389020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (8469, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('nlpa.csv')\n",
        "import re\n",
        "\n",
        "def clean_description(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text=str(text)\n",
        "    text=text.lower()\n",
        "    text=re.sub(r'\\{[^}}]+\\}', '', text)\n",
        "\n",
        "    text= re.sub(r'\\d+\\.\\d+\\.\\d+', '',  text)\n",
        "    text= re.sub(r'\\d+\\.\\d+\\.\\d+', '', text)\n",
        "    text= re.sub(r'please assist', '', text)\n",
        "\n",
        "    text= re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    text= ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "df['cleaned_description']= df['Ticket Description'].apply(clean_description)\n",
        "cleaned_df= df[['cleaned_description', 'Ticket Type']]\n",
        "cleaned_df.to_csv('rp.csv', index=False)"
      ],
      "metadata": {
        "id": "IJOercpuEqYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "custom_nltk_dir = './custom_nltk_data'\n",
        "os.makedirs(custom_nltk_dir, exist_ok=True)\n",
        "nltk.data.path.append(custom_nltk_dir)\n",
        "nltk.download('punkt', download_dir=custom_nltk_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBPNvsi9IXuY",
        "outputId": "116437d4-67c2-45b6-cb1a-40809beff7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to ./custom_nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('rp.csv')\n",
        "\n",
        "def basic_tokenize(text):\n",
        "    sentences = text.split('.')\n",
        "    words = text.split()\n",
        "    return {\n",
        "        'sentences': [s.strip() for s in sentences if s.strip()],\n",
        "        'words': words\n",
        "    }\n",
        "\n",
        "\n",
        "df['tokenized'] = df['cleaned_description'].apply(basic_tokenize)\n",
        "df['sentence_tokens'] = df['tokenized'].apply(lambda x: x['sentences'])\n",
        "df['word_tokens'] = df['tokenized'].apply(lambda x: x['words'])\n",
        "\n",
        "df.to_csv('tokenizedt.csv', index=False)"
      ],
      "metadata": {
        "id": "Na13NDTuUiDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('tokenizedt.csv')\n",
        "\n",
        "import ast\n",
        "df['word_tokens'] = df['word_tokens'].apply(ast.literal_eval)\n",
        "\n",
        "def clean_text(tokens):\n",
        "   return [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(tokens):\n",
        "   return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(tokens):\n",
        "   return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "\n",
        "df['cleaned_tokens'] = df['word_tokens'].apply(clean_text)\n",
        "df['tokens_no_stop'] = df['cleaned_tokens'].apply(remove_stopwords)\n",
        "df['lemmatized'] = df['tokens_no_stop'].apply(lemmatize_text)\n",
        "\n",
        "df.to_csv('new_text.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdAnfbnLXB_B",
        "outputId": "602bf8e4-1824-4093-d5d5-bdd0f7af08b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "df= pd.read_csv('new_text.csv')\n",
        "\n",
        "df['lemmatized'] = df['lemmatized'].apply(ast.literal_eval)\n",
        "\n",
        "model = Word2Vec(sentences=df['lemmatized'],\n",
        "                vector_size=200,\n",
        "                window=5,\n",
        "                min_count=2,\n",
        "                epochs=5,\n",
        "                workers=4)\n",
        "\n",
        "def get_doc_vector(tokens):\n",
        "    vec = np.zeros(200)\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        if word in model.wv:\n",
        "            vec += model.wv[word]\n",
        "            count += 1\n",
        "    return vec / count if count != 0 else vec\n",
        "\n",
        "\n",
        "df['document_vectors'] = df['lemmatized'].apply(get_doc_vector)\n",
        "\n",
        "model.save(\"word2vec_model.model\")\n",
        "df.to_csv('embedded_data.csv', index=False)"
      ],
      "metadata": {
        "id": "vd7gqaN8c3WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv('embedded_data.csv')\n",
        "\n",
        "def string_to_array(vector_string):\n",
        "    try:\n",
        "        nums = vector_string.strip('[]').split()\n",
        "        return np.array([float(num) for num in nums])\n",
        "    except:\n",
        "        return np.zeros(100)\n",
        "\n",
        "df['document_vectors'] = df['document_vectors'].apply(string_to_array)\n",
        "\n",
        "X = np.stack(df['document_vectors'].values)\n",
        "y = df['Ticket Type']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "EfAJFqaTckZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "joblib.dump(clf, 'random_forest_model.joblib')\n",
        "print(\"Model saved successfully\")\n",
        "\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKLTi8EHmKtw",
        "outputId": "13cbd021-d36e-4638-ecc1-9add50fda1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully\n",
            "Accuracy: 0.20838252656434475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class distribution in training data:\")\n",
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YIcVVJ8rOOq",
        "outputId": "55f9c20f-bd62-4d85-dc99-42daa6f71295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training data:\n",
            "Ticket Type\n",
            "Technical issue         1410\n",
            "Refund request          1390\n",
            "Cancellation request    1356\n",
            "Billing inquiry         1317\n",
            "Product inquiry         1302\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [word.strip('.,!?()[]{}:;') for word in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "def get_doc_vector(tokens):\n",
        "    vec = np.zeros(200)\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        if word in model.wv:\n",
        "            vec += model.wv[word]\n",
        "            count += 1\n",
        "    return vec / count if count != 0 else vec\n",
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"word2vec_model.model\")"
      ],
      "metadata": {
        "id": "lOKeX7iuPYD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"I need a refund for my purchase\",\n",
        "    \"Device not turning on after charging\",\n",
        "    \"Cancel my subscription immediately\",\n",
        "    \"Technical error when launching app\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = simple_preprocess(text)\n",
        "    vector = get_doc_vector(tokens)\n",
        "    prediction = clf.predict([vector])\n",
        "    print(f\"Text: {text}\\nPredicted Type: {prediction[0]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVSFlFhhQS42",
        "outputId": "5153564e-0393-4fbe-db38-96083e5416e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I need a refund for my purchase\n",
            "Predicted Type: Refund request\n",
            "\n",
            "Text: Device not turning on after charging\n",
            "Predicted Type: Billing inquiry\n",
            "\n",
            "Text: Cancel my subscription immediately\n",
            "Predicted Type: Cancellation request\n",
            "\n",
            "Text: Technical error when launching app\n",
            "Predicted Type: Refund request\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('customer_support_tickets.csv')\n",
        "df_time = df[['First Response Time', 'Time to Resolution']]\n",
        "df_time.to_csv('resolution_time_only.csv', index=False)"
      ],
      "metadata": {
        "id": "-tsEHd5JSUNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import joblib\n",
        "\n",
        "df = pd.read_csv('resolution_time_only.csv')\n",
        "\n",
        "df['First Response Time'] = pd.to_datetime(df['First Response Time']).astype(np.int64) // 10**9\n",
        "df['Time to Resolution'] = pd.to_datetime(df['Time to Resolution']).astype(np.int64) // 10**9\n",
        "\n",
        "df['Resolution_Time_Minutes'] = (df['Time to Resolution'] - df['First Response Time'])/60\n",
        "df = df.dropna()\n",
        "\n",
        "X = df[['First Response Time']]\n",
        "y = df['Resolution_Time_Minutes']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "\n",
        "joblib.dump(model, 'resolution_time_model.joblib')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u3otmJtTFYd",
        "outputId": "2d6f9667-cbd8-485b-dfdc-c61fdd283823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.06843307873601723\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resolution_time_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "model = joblib.load('resolution_time_model.joblib')\n",
        "\n",
        "def predict_resolution_time(first_response_time):\n",
        "   first_response_timestamp = pd.to_datetime(first_response_time).timestamp()\n",
        "   prediction = model.predict([[first_response_timestamp]])\n",
        "   return prediction[0]\n",
        "\n",
        "test_time = \"2024-02-04 10:00:00\"\n",
        "predicted_minutes = predict_resolution_time(test_time)\n",
        "print(f\"Predicted resolution time in minutes: {predicted_minutes}\")\n",
        "\n",
        "test_times = [\n",
        "   \"2024-02-04 09:00:00\",\n",
        "   \"2024-02-04 14:30:00\",\n",
        "   \"2024-02-04 18:45:00\"\n",
        "]\n",
        "\n",
        "for time in test_times:\n",
        "   prediction = predict_resolution_time(time)\n",
        "   print(f\"Time: {time} -> Predicted minutes: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37TcT45SUiK-",
        "outputId": "a3da04e0-d69b-4b53-ef77-a2465d71e3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted resolution time in minutes: -56970221.03052776\n",
            "Time: 2024-02-04 09:00:00 -> Predicted minutes: -56970221.03052776\n",
            "Time: 2024-02-04 14:30:00 -> Predicted minutes: -56970221.03052776\n",
            "Time: 2024-02-04 18:45:00 -> Predicted minutes: -56970221.03052776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkASgFLjVWUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}